{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2194617c",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf07255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c651a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "MODEL_NAME = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, output_hidden_states = True).to(DEVICE).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5568bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitlens(model, tokenizer, prompt, top_k=5, device=DEVICE):\n",
    "    #token -> tensor\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    ##forward pass to get all hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)   \n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    W_U = model.lm_head.weight\n",
    "    final_norm = model.transformer.ln_f\n",
    "    results = []\n",
    "\n",
    "    for layer_idx in range(1, len(hidden_states)):\n",
    "        h = hidden_states[layer_idx]\n",
    "        h_last = h[0, -1]   \n",
    "        \n",
    "        h_norm = final_norm(h_last)\n",
    "\n",
    "        logits = torch.matmul(W_U, h_norm)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        top_probs, top_indices = torch.topk(probs, k=top_k)\n",
    "        top_tokens = [tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "\n",
    "        results.append({\n",
    "            \"layer\": layer_idx,\n",
    "            \"tokens\": top_tokens,\n",
    "            \"probs\": top_probs.cpu().tolist()\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17424729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logitlens_results(results, prompt):\n",
    "    print(f\"\\nPrompt: {prompt}\\n\")\n",
    "    for r in results:\n",
    "        layer = r[\"layer\"]\n",
    "        toks = r[\"tokens\"]\n",
    "        probs = r[\"probs\"]\n",
    "\n",
    "        pretty = [f\"{tok.strip() or repr(tok)} ({p:.3f})\"\n",
    "                  for tok, p in zip(toks, probs)]\n",
    "        print(f\"Layer {layer:2d}: \" + \" | \".join(pretty))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a993c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The capital of France is\n",
      "\n",
      "Layer  1: not (0.332) | now (0.128) | still (0.082) | also (0.073) | a (0.038)\n",
      "Layer  2: now (0.271) | not (0.256) | also (0.156) | still (0.081) | unlikely (0.017)\n",
      "Layer  3: now (0.366) | not (0.172) | also (0.152) | still (0.092) | currently (0.028)\n",
      "Layer  4: now (0.515) | also (0.145) | not (0.126) | currently (0.040) | still (0.038)\n",
      "Layer  5: now (0.627) | not (0.080) | also (0.053) | still (0.047) | already (0.033)\n",
      "Layer  6: now (0.552) | still (0.109) | not (0.088) | also (0.068) | already (0.030)\n",
      "Layer  7: now (0.572) | not (0.123) | still (0.056) | also (0.050) | a (0.016)\n",
      "Layer  8: now (0.689) | also (0.063) | still (0.059) | not (0.048) | already (0.025)\n",
      "Layer  9: now (0.423) | located (0.145) | also (0.071) | not (0.070) | still (0.049)\n",
      "Layer 10: France (0.621) | Paris (0.182) | now (0.039) | located (0.029) | French (0.011)\n",
      "Layer 11: France (0.242) | Paris (0.139) | now (0.103) | the (0.042) | located (0.023)\n",
      "Layer 12: the (0.038) | a (0.022) | , (0.019) | in (0.018) | to (0.012)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "results = logitlens(model, tokenizer, prompt, top_k=5)\n",
    "print_logitlens_results(results, prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
